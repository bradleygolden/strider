if Code.ensure_loaded?(ReqLLM) do
  defmodule Strider.Backends.ReqLLM do
    @moduledoc """
    Backend implementation using the ReqLLM library.

    ReqLLM provides a unified interface to multiple LLM providers including
    OpenAI, Anthropic, Google, Amazon Bedrock, OpenRouter, and more.

    ## Model Specification

    The model must include the provider in `"provider:model"` format:

        # Direct API access
        model: "anthropic:claude-sonnet-4-5"
        model: "openai:gpt-4"
        model: "google:gemini-1.5-pro"

        # Via OpenRouter (access many models through one API)
        model: "openrouter:anthropic/claude-sonnet-4-5"
        model: "openrouter:openai/gpt-4"

        # Via Amazon Bedrock
        model: "amazon_bedrock:anthropic.claude-sonnet-4-5-20241022-v2:0"

        # Via Google Vertex AI
        model: "google_vertex:claude-sonnet-4-5@20240620"

    ## Configuration

    Additional options can be passed as keyword arguments:

    - `:temperature` - Sampling temperature (0.0 to 2.0)
    - `:max_tokens` - Maximum tokens in response
    - `:top_p` - Nucleus sampling parameter
    - `:stop` - Stop sequences

    ## Examples

        # Using Anthropic directly
        agent = Strider.Agent.new(
          {Strider.Backends.ReqLLM, "anthropic:claude-sonnet-4-5"},
          temperature: 0.7,
          max_tokens: 1000
        )

        # Using OpenRouter
        agent = Strider.Agent.new(
          {Strider.Backends.ReqLLM, "openrouter:anthropic/claude-sonnet-4-5"}
        )

        # Using Amazon Bedrock
        agent = Strider.Agent.new(
          {Strider.Backends.ReqLLM, "amazon_bedrock:anthropic.claude-sonnet-4-5-20241022-v2:0"}
        )

    ## Supported Providers

    See ReqLLM documentation for the full list. Common providers:

    - `:anthropic` - Anthropic API (Claude models)
    - `:openai` - OpenAI API (GPT models)
    - `:google` - Google AI (Gemini models)
    - `:openrouter` - OpenRouter (multi-provider gateway)
    - `:amazon_bedrock` - AWS Bedrock
    - `:google_vertex` - Google Vertex AI
    - `:groq` - Groq (fast inference)
    - `:cerebras` - Cerebras

    """

    @behaviour Strider.Backend

    alias ReqLLM.Message.ContentPart
    alias Strider.Content.Part
    alias Strider.{Message, Response}

    @impl true
    def call(config, messages, opts) do
      model = Map.fetch!(config, :model)
      options = build_options(config)
      output_schema = Keyword.get(opts, :output_schema)
      req_messages = Enum.map(messages, &to_req_llm_message/1)

      if output_schema do
        call_with_schema(model, req_messages, output_schema, options)
      else
        call_text(model, req_messages, options)
      end
    end

    defp call_text(model, req_messages, options) do
      case ReqLLM.generate_text(model, req_messages, options) do
        {:ok, response} -> {:ok, normalize_response(response, model)}
        {:error, reason} -> {:error, reason}
      end
    end

    defp call_with_schema(model, messages, output_schema, options) do
      # Convert struct schemas to object schemas for LLM (struct schemas can't be JSON encoded)
      llm_schema = to_llm_schema(output_schema)

      case ReqLLM.generate_object(model, messages, llm_schema, options) do
        {:ok, response} -> {:ok, normalize_object_response(response, model, output_schema)}
        {:error, reason} -> {:error, reason}
      end
    end

    defp to_llm_schema(%Zoi.Types.Struct{fields: fields}), do: Zoi.object(fields)
    defp to_llm_schema(schema), do: schema

    @impl true
    def stream(config, messages, _opts) do
      model = Map.fetch!(config, :model)
      options = build_options(config)
      req_messages = Enum.map(messages, &to_req_llm_message/1)

      case ReqLLM.stream_text(model, req_messages, options) do
        {:ok, stream_response} ->
          text_stream =
            stream_response
            |> ReqLLM.StreamResponse.tokens()
            |> Stream.map(fn text ->
              %{content: text, metadata: %{backend: :req_llm}}
            end)

          {:ok, text_stream}

        {:error, reason} ->
          {:error, reason}
      end
    end

    @impl true
    def introspect(config) do
      model = Map.get(config, :model, "unknown")
      {provider, _model_name} = parse_model_string(model)

      %{
        provider: provider,
        model: model,
        operation: :chat,
        capabilities: [:streaming, :multi_provider]
      }
    end

    # Private helpers

    defp build_options(config) do
      config
      |> Map.take([:temperature, :max_tokens, :top_p, :stop, :req_http_options])
      |> Enum.reject(fn {_k, v} -> is_nil(v) end)
      |> Enum.into([])
    end

    defp normalize_response(response, model) do
      content = extract_content(response)
      usage = ReqLLM.Response.usage(response) || %{}
      finish_reason = ReqLLM.Response.finish_reason(response)

      Response.new(
        content: content,
        finish_reason: normalize_finish_reason(finish_reason),
        usage: normalize_usage(usage),
        metadata: build_metadata(response, model, finish_reason)
      )
    end

    defp normalize_object_response(response, model, output_schema) do
      object = ReqLLM.Response.object(response)
      content = maybe_parse_struct(output_schema, object)
      usage = ReqLLM.Response.usage(response) || %{}
      finish_reason = ReqLLM.Response.finish_reason(response)

      Response.new(
        content: content,
        finish_reason: normalize_finish_reason(finish_reason),
        usage: normalize_usage(usage),
        metadata: build_metadata(response, model, finish_reason)
      )
    end

    defp maybe_parse_struct(%Zoi.Types.Struct{} = schema, object) when is_map(object) do
      case Zoi.parse(schema, object) do
        {:ok, struct} -> struct
        {:error, _} -> object
      end
    end

    defp maybe_parse_struct(_schema, object), do: object

    defp build_metadata(response, model, raw_finish_reason) do
      {provider, _model_name} = parse_model_string(model)

      %{
        provider: provider,
        model: response.model,
        response_id: response.id,
        raw_finish_reason: raw_finish_reason
      }
      |> Enum.reject(fn {_k, v} -> is_nil(v) end)
      |> Map.new()
    end

    defp parse_model_string(model) when is_binary(model) do
      case String.split(model, ":", parts: 2) do
        [provider, model_name] -> {provider, model_name}
        [model_name] -> {"unknown", model_name}
      end
    end

    defp extract_content(%ReqLLM.Response{message: nil}), do: nil
    defp extract_content(%ReqLLM.Response{message: %{content: content}}), do: content

    defp normalize_finish_reason(:stop), do: :stop
    defp normalize_finish_reason(:length), do: :max_tokens
    defp normalize_finish_reason(:tool_calls), do: :tool_use
    defp normalize_finish_reason(:content_filter), do: :content_filter
    defp normalize_finish_reason(:error), do: :error
    defp normalize_finish_reason(nil), do: nil

    defp normalize_usage(usage) when is_map(usage) do
      %{
        input_tokens: Map.get(usage, :input_tokens, 0),
        output_tokens: Map.get(usage, :output_tokens, 0)
      }
    end

    defp normalize_usage(_), do: %{}

    # Convert Strider.Message → ReqLLM message format
    # Single text → use plain map with string content (loose map)
    # Multi-part → use ReqLLM.Message struct with ContentPart list
    defp to_req_llm_message(%Message{role: role, content: [%Part{type: :text, text: text}]}) do
      %{role: role, content: text}
    end

    defp to_req_llm_message(%Message{role: role, content: parts}) do
      content_parts = Enum.map(parts, &to_req_llm_part/1)
      %ReqLLM.Message{role: role, content: content_parts}
    end

    defp to_req_llm_part(%Part{type: :text, text: text}) do
      ContentPart.text(text)
    end

    defp to_req_llm_part(%Part{type: :image_url, url: url}) do
      ContentPart.image_url(url)
    end

    defp to_req_llm_part(%Part{type: :image, data: data, media_type: media_type}) do
      ContentPart.image(data, media_type)
    end

    defp to_req_llm_part(%Part{
           type: :file,
           data: data,
           filename: filename,
           media_type: media_type
         }) do
      ContentPart.file(data, filename, media_type)
    end

    defp to_req_llm_part(%Part{type: type} = part) do
      # Fallback for other types (audio, video, custom) - pass as map
      part |> Map.from_struct() |> Map.delete(:metadata) |> Map.put(:type, type)
    end
  end
end
